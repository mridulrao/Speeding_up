{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "013ae6cf-b821-4ab9-9b04-66053b6a6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset(\"abisee/cnn_dailymail\", '1.0.0', split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b4541266-a228-43fd-98fa-1e196c87e8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 287113\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a00d648f-bf17-41de-aa34-970b2671375e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28712"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.shard(num_shards=10, index=0)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "03db2ffb-b0b1-471d-9225-fff1ab4a43e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers, processors\n",
    "\n",
    "# tokenizer = Tokenizer(models.BPE())\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "# trainer = trainers.BpeTrainer(vocab_size=30000, special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "\n",
    "# dataset = [article['article'] for article in train_dataset]\n",
    "# tokenizer.train_from_iterator(dataset, trainer)\n",
    "# tokenizer.save(\"bpe_tokenizer.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fda36849-e9b0-4497-a197-d1f10b6a0c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_pad(sentence, tokenizer, max_len):\n",
    "    tokens = tokenizer.encode(sentence).ids  # Tokenize the sentence\n",
    "    if len(tokens) < max_len:\n",
    "        tokens = tokens + [tokenizer.token_to_id(\"<pad>\")] * (max_len - len(tokens))  # Pad\n",
    "    else:\n",
    "        tokens = tokens[:max_len]  # Truncate if necessary\n",
    "    return torch.tensor(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5c4faaac-ddbc-433d-8b66-acbafc598e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"bpe_tokenizer.json\")\n",
    "\n",
    "MAX_LEN = 512 \n",
    "SUMMARY_MAX_LEN = 128\n",
    "\n",
    "article = train_dataset[0]['article']\n",
    "summary = train_dataset[0]['highlights']\n",
    "\n",
    "\n",
    "article = tokenize_and_pad(article, tokenizer, MAX_LEN)\n",
    "summary = tokenize_and_pad(summary, tokenizer, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fda269bd-9c38-491a-be67-eb4db58e4f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(train_dataset):\n",
    "    train_dataset['input_ids'] = [tokenize_and_pad(article, tokenizer, MAX_LEN) for article in train_dataset['article']]\n",
    "    train_dataset['labels'] = [tokenize_and_pad(summary, tokenizer, SUMMARY_MAX_LEN) for summary in train_dataset['highlights']]\n",
    "    \n",
    "    return train_dataset\n",
    "    \n",
    "train_dataset = train_dataset.map(preprocess_batch, batched = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a713803f-3e6c-4387-b824-a279518f5f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'labels'],\n",
       "    num_rows: 28712\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "021b4922-baa1-4cfc-8007-aafa731f3145",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2704357-0159-44f2-a602-91c711133301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id', 'input_ids', 'labels'],\n",
       "    num_rows: 28712\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6bcd9b1e-69c1-4d5f-b648-1103f9697ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "11620bfc-dddc-4f54-9145-344dfb916a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, enc_hidden_dim, dec_hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn1 = nn.LSTM(emb_dim, enc_hidden_dim, batch_first=True)\n",
    "        self.rnn2 = nn.LSTM(enc_hidden_dim, enc_hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hidden_dim, dec_hidden_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src) \n",
    "        outputs1, (hidden1, cell1) = self.rnn1(embedded)\n",
    "        outputs2, (hidden2, cell2) = self.rnn2(outputs1)\n",
    "        hidden2 = torch.tanh(self.fc(hidden2))\n",
    "        return outputs2, hidden2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88b1b16f-5971-4330-9538-23e8f97ebb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        # The decoder hidden state is repeated src_len times (once for each time step in the encoder's output). \n",
    "        # The purpose is to compare the decoder hidden state to each of the encoder's hidden states at each time step.\n",
    "        hidden = hidden.repeat(src_len, 1, 1).transpose(0, 1)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        \n",
    "        v = self.v.repeat(encoder_outputs.shape[0], 1).unsqueeze(1)\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        return torch.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "15ce441e-6651-42f1-82b3-5185173619af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, enc_hidden_dim, dec_hidden_dim, attention):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn1 = nn.LSTM(enc_hidden_dim + emb_dim, dec_hidden_dim, batch_first=True)\n",
    "        self.rnn2 = nn.LSTM(dec_hidden_dim, dec_hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(enc_hidden_dim + dec_hidden_dim + emb_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_, hidden, encoder_outputs):\n",
    "        # unsqueeze to [batch_size, 1]\n",
    "        input_ = input_.unsqueeze(1)\n",
    "\n",
    "        embedded = self.embedding(input_)\n",
    "\n",
    "        attn_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs) # attention weights * encoder output\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, attn_applied), dim=2)\n",
    "        \n",
    "        if isinstance(hidden, torch.Tensor):\n",
    "            hidden = (hidden, torch.zeros_like(hidden))\n",
    "            \n",
    "        output1, (hidden1, cell1) = self.rnn1(rnn_input, hidden)\n",
    "        \n",
    "        if isinstance(hidden1, torch.Tensor):\n",
    "            hidden1 = (hidden1, torch.zeros_like(hidden1))\n",
    "            \n",
    "        output2, (hidden2, cell2) = self.rnn2(output1, hidden1)\n",
    "\n",
    "        prediction = self.fc_out(torch.cat((output2.squeeze(1), attn_applied.squeeze(1), embedded.squeeze(1)), dim=1))\n",
    "        \n",
    "        return prediction, hidden2\n",
    "\n",
    "\n",
    "# https://discuss.pytorch.org/t/the-difference-and-use-of-output-and-hidden-state-of-an-rnn/15108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69580549-7f00-47b4-8485-1a0c86863cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        trg_len = trg.shape[1]\n",
    "        batch_size = src.shape[0]\n",
    "        outputs = torch.zeros(batch_size, trg_len, self.decoder.fc_out.out_features).to(src.device)\n",
    "\n",
    "        #first decoder input\n",
    "        input_ = trg[:, 0]\n",
    "\n",
    "        #loop until max length of output\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input_, hidden, encoder_outputs)\n",
    "            outputs[:, t, :] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input_ = trg[:, t] if random.random() < teacher_forcing_ratio else top1\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06502b5e-c7f9-43fb-87e7-cb865aa0b142",
   "metadata": {},
   "source": [
    "## LSTM Explanation\n",
    "time step - Single point in a sequence of data \n",
    "layer - depth of LSTM in terms of stacked LSTM layers \n",
    "output - Output of LSTM for each time step in input sequence. Contains the hidden state for all time steps in sequence of all layers(default we have 1 layer). If hidden state > 1, output contains hidden states from last layer only \n",
    "hidden - hidden state at the last time step\n",
    "cell - memory state of LSTM at last time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c2dbaeef-f75e-47e1-b6f0-0b6391c6d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Hyperparameters\n",
    "INPUT_DIM = len(tokenizer.get_vocab())\n",
    "OUTPUT_DIM = len(tokenizer.get_vocab())\n",
    "EMB_DIM = 512\n",
    "ENC_HIDDEN_DIM = 512\n",
    "DEC_HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 0.001\n",
    "MAX_LEN = 512 \n",
    "SUMMARY_MAX_LEN = 128\n",
    "\n",
    "# Instantiate the model\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, ENC_HIDDEN_DIM, DEC_HIDDEN_DIM)\n",
    "attention = Attention(ENC_HIDDEN_DIM, DEC_HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, ENC_HIDDEN_DIM, DEC_HIDDEN_DIM, attention)\n",
    "\n",
    "# Training function\n",
    "def train(model, iterator, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    # Define optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.token_to_id(\"<pad>\"))\n",
    "    \n",
    "    for i, batch in tqdm(enumerate(iterator)):\n",
    "        src = batch['input_ids'].to(device)\n",
    "        trg = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        # Reshape the output and target tensors\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f702c3-3fb0-45e7-8fe8-cf197bc25aa2",
   "metadata": {},
   "source": [
    "# Data parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4a9ffd3f-6102-41a8-b6c6-316b689e5835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1795it [49:56,  1.67s/it]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('Agg')\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "\n",
    "# single GPU\n",
    "\n",
    "num_repeat = 1\n",
    "CLIP = 1\n",
    "stmt = '''\n",
    "device = 'cuda:0'\n",
    "train(model, train_loader, CLIP, device)\n",
    "'''\n",
    "\n",
    "setup_1 = '''\n",
    "device = 'cuda:0'\n",
    "model = Seq2Seq(encoder, decoder).to(device)\n",
    "'''\n",
    "\n",
    "sg_run_times = timeit.repeat(\n",
    "    stmt, setup_1, number=1, repeat=num_repeat, globals=globals())\n",
    "sg_mean, sg_std = np.mean(sg_run_times), np.std(sg_run_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e7d05a9e-8581-4373-9f23-0330e15fe97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2996.7662307778373"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87828a6d-f0f4-4782-86d0-9b67192d4fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1795it [42:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# data parallelization \n",
    "\n",
    "num_repeat = 1\n",
    "CLIP = 1\n",
    "stmt = '''\n",
    "device = 'cuda'\n",
    "train(model, train_loader, CLIP, device)\n",
    "'''\n",
    "\n",
    "setup_2 = '''\n",
    "device = 'cuda'\n",
    "\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "  model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "'''\n",
    "\n",
    "dp_run_times = timeit.repeat(\n",
    "    stmt, setup_2, number=1, repeat=num_repeat, globals=globals())\n",
    "dp_mean, dp_std = np.mean(dp_run_times), np.std(dp_run_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "24205044-af0e-46dd-96a1-ff539856b76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2520.746455488028"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6b62027-576d-4a73-afa9-45534af95694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(means, stds, labels, fig_name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(np.arange(len(means)), means, yerr=stds,\n",
    "           align='center', alpha=0.5, ecolor='red', capsize=10, width=0.6)\n",
    "    ax.set_ylabel('Model Execution Time')\n",
    "    ax.set_xticks(np.arange(len(means)))\n",
    "    ax.set_xticklabels(labels)\n",
    "    ax.yaxis.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_name)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "plot([sg_mean, dp_mean],\n",
    "     [sg_std, dp_std],\n",
    "     ['Single GPU', 'Data Parallelization'],\n",
    "     'sg_vs_dp.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702414f9-c32b-4286-a91b-936f5e0f6f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
